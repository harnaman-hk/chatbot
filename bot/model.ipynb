{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzvdLM6hR7ZQ",
        "outputId": "cc22a960-82fa-466d-91fd-f8c8284d864f"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AIK9f5TSWWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01b7288-086e-4fb8-d2f6-46d81f0282da"
      },
      "source": [
        "pip install tflearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biM0srwSSdFO",
        "outputId": "91f83c9c-4bf0-48e6-dfe5-ce4dd13155e7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "import tflearn\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import string\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV432KscWC8n"
      },
      "source": [
        "stemmer = LancasterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P7c0eEETI6Z"
      },
      "source": [
        "with open(\"greetings.json\") as file:\n",
        "  data = json.load(file)\n",
        "\n",
        "with open(\"coding1.json\") as file:\n",
        "  data[\"intents\"].extend(json.load(file)[\"intents\"])\n",
        "\n",
        "with open(\"coding2.json\") as file:\n",
        "  data[\"intents\"].extend(json.load(file)[\"intents\"])\n",
        "\n",
        "with open(\"coding3.json\") as file:\n",
        "  data[\"intents\"].extend(json.load(file)[\"intents\"])\n",
        "\n",
        "with open(\"collegequeries.json\") as file:\n",
        "  data[\"intents\"].extend(json.load(file)[\"intents\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxw9KVJETT5A"
      },
      "source": [
        "try:\n",
        "  with open(\"data.pickle\", \"rb\") as f:\n",
        "    words, labels, training, output = pickle.load(f)\n",
        "\n",
        "except:\n",
        "  words =[]\n",
        "  labels = []\n",
        "  train_x = []\n",
        "  train_y = []\n",
        "\n",
        "  for intent in data['intents']:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "      # do stemming -- bring the sentence to root word(s)\n",
        "      wrds = nltk.word_tokenize(pattern)\n",
        "      words.extend(wrds)\n",
        "\n",
        "      # add pattern of words\n",
        "      train_x.append(wrds)\n",
        "      train_y.append(intent[\"tag\"])\n",
        "\n",
        "    if intent[\"tag\"] not in labels:\n",
        "      labels.append(intent[\"tag\"])\n",
        "\n",
        "  # delete duplicates\n",
        "  words = [stemmer.stem(w.translate(str.maketrans('', '', string.punctuation)).lower()) for w in words if w.translate(str.maketrans('', '', string.punctuation)).isalpha()]\n",
        "  re_puncts = re.compile('[%s]'% re.escape(string.punctuation))\n",
        "  words = [re_puncts.sub('', w) for w in words]\n",
        "  words = [w for w in words if w.isalpha()]\n",
        "  words = sorted(set(words))\n",
        "  training = []\n",
        "  output = []\n",
        "  out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "  for i, xi in enumerate(train_x):\n",
        "    bag = []\n",
        "\n",
        "    wrds = [stemmer.stem(w) for w in xi]\n",
        "    for w in words:\n",
        "      if w in wrds:\n",
        "        bag.append(1)\n",
        "      else :\n",
        "        bag.append(0)\n",
        "\n",
        "    output_row = out_empty.copy()\n",
        "    output_row[labels.index(train_y[i])] = 1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)\n",
        "\n",
        "  training = np.array(training)\n",
        "  output = np.array(output)\n",
        "\n",
        "  with open(\"data.pickle\", \"wb\") as f:\n",
        "    pickle.dump((words, labels, training, output), f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp3Tg26IyfRy",
        "outputId": "d27371d3-5343-4892-c1fd-a771d6491454"
      },
      "source": [
        "len(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "262"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rf2-IVvaMbF"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI48j44rYiU9",
        "outputId": "dca2a120-51e1-4647-c1e3-a793c2b6228a"
      },
      "source": [
        "# ------------- modelling ------------\n",
        "try:\n",
        "  model.load(\"model.tflearn\")\n",
        "\n",
        "except:\n",
        "  ops.reset_default_graph()\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10)\n",
        "  net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "  net = tflearn.fully_connected(net, 8) # hidden layer\n",
        "  net = tflearn.fully_connected(net, 8) # hidden layer\n",
        "  net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\") # output layer\n",
        "  net = tflearn.regression(net)\n",
        "\n",
        "  model = tflearn.DNN(net)\n",
        "  model.fit(training, output, n_epoch=500, batch_size=10, show_metric=True)\n",
        "  # model.summary()\n",
        "  model.save(\"model.tflearn\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Step: 13499  | total loss: \u001b[1m\u001b[32m0.00681\u001b[0m\u001b[0m | time: 0.151s\n",
            "| Adam | epoch: 500 | loss: 0.00681 - acc: 0.9950 -- iter: 260/262\n",
            "Training Step: 13500  | total loss: \u001b[1m\u001b[32m0.00617\u001b[0m\u001b[0m | time: 0.156s\n",
            "| Adam | epoch: 500 | loss: 0.00617 - acc: 0.9955 -- iter: 262/262\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF4br5nliJ4v"
      },
      "source": [
        "def vectorize(x, words):\n",
        "  vector = [0 for _ in range(len(words))]\n",
        "\n",
        "  s_words = nltk.word_tokenize(x)\n",
        "  s_words = [stemmer.stem(w.lower()) for w in s_words]\n",
        "  re_puncts = re.compile('[%s]'% re.escape(string.punctuation))\n",
        "  s_words = [re_puncts.sub('', w) for w in s_words]\n",
        "  s_words = [w for w in s_words if w.isalpha()]\n",
        "\n",
        "  for se in s_words:\n",
        "    for i, w in enumerate(words):\n",
        "      if w == se:\n",
        "        vector[i] = 1\n",
        "\n",
        "  return np.array(vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPQ3OwVOi603"
      },
      "source": [
        "def demo_chat():\n",
        "  '''\n",
        "  temporary function for local trial of chatbot\n",
        "  '''\n",
        "  print(\"Chatter away\")\n",
        "\n",
        "  while True:\n",
        "    inp = input(\"You : \")\n",
        "    if inp.lower() == \"break\" :\n",
        "      break\n",
        "    \n",
        "    results = model.predict([vectorize(inp, words)])\n",
        "    results_index = np.argmax(results)\n",
        "    tag = labels[results_index]\n",
        "    \n",
        "    if results[0][results_index] > 0.7:\n",
        "      for tg in data[\"intents\"]:\n",
        "        if tg[\"tag\"] == tag:\n",
        "          responses = tg[\"responses\"]\n",
        "\n",
        "      print(random.choice(responses))\n",
        "\n",
        "    else:\n",
        "      print(\"Are you messing with me?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipAd2zgDkKp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2065be13-512a-4527-86d5-da0930b1782e"
      },
      "source": [
        "demo_chat()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chatter away\n",
            "You : break\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}